# -*- coding: utf-8 -*-
"""kpfSBERT_clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b1o_MZwjuuXL6xZMJIAppErF-Ug1tyL2

# 사전 준비 코드
"""

from sentence_transformers import SentenceTransformer
from datetime import datetime
import logging
import re
import copy
import numpy as np
import pandas as pd
from kiwipiepy import Kiwi
import hdbscan
import umap
import pymysql
import mysql.connector
import json
import ast
import itertools
import os
import hanja
import matplotlib.pyplot as plt

#kpfSBERT 모델 로딩
model_path = "bongsoo/kpf-sbert-v1.1" # sbert
cluster_mode = 'summary'
model = SentenceTransformer(model_path)

# UMAP 차원축소 함수
def umap_process(corpus_embeddings, n_components=5):
    umap_embeddings = umap.UMAP(n_neighbors=15,
                                n_components=n_components,
                                metric='cosine').fit_transform(corpus_embeddings)
    return umap_embeddings

# HDBSCAN 함수
def hdbscan_process(corpus, corpus_embeddings, min_cluster_size=5, min_samples=7, umap=True, n_components=5, method='eom'):

    if umap:
        umap_embeddings = umap_process(corpus_embeddings, n_components)
    else:
        umap_embeddings = corpus_embeddings

    cluster = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,
                              min_samples=min_samples,
                              allow_single_cluster=True,
                              metric='euclidean',
                              core_dist_n_jobs=1,# knn_data = Parallel(n_jobs=self.n_jobs, max_nbytes=None) in joblib
                              cluster_selection_method=method).fit(umap_embeddings) #eom leaf

    docs_df = pd.DataFrame(corpus, columns=["Doc"])
    docs_df['Topic'] = cluster.labels_
    docs_df['Doc_ID'] = range(len(docs_df))
    docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})

    return docs_df, docs_per_topic, cluster

# 형태소 분석 후처리 함수 : 명사 추출
kiwi = Kiwi()

def noun_extractor(text: str):
    results = []
    result = kiwi.analyze(text)
    for token, pos, _, _ in result[0][0]:
      if token not in stopwords:
        if len(token) != 1:
          if pos.startswith('N') or pos.startswith('SL'):
            results.append(token)

    return results

def extract_topic_sizes(df):
    topic_sizes = (df.groupby(['Topic'])
                     .Doc
                     .count()
                     .reset_index()
                     .rename({"Topic": "Topic", "Doc": "Size"}, axis='columns')
                     .sort_values("Size", ascending=False))
    return topic_sizes

# 클러스터별 대표 기사 선택 함수
def best_title(titles: list, cluster_kws: list):
    # 각 제목에서 명사 추출
    titles_nouns = [noun_extractor(title) for title in titles]

    # 각 제목의 명사 리스트와 클러스터 키워드 리스트 간에 공통된 단어 개수를 세어주는 함수
    def count_common_words(nouns, kws):
        return len(set(nouns) & set(kws))

    max_common_count = 0
    best_title_index = None

    for i, nouns in enumerate(titles_nouns):
        common_count = count_common_words(nouns, cluster_kws)
        if common_count > max_common_count:
            max_common_count = common_count
            best_title_index = i

    if best_title_index is not None:
        return titles[best_title_index]
    else:
        return None  # 적절한 제목을 찾지 못한 경우


# 키워드 추출 함수 import
from kpf_keybert import extract_kws

# DB config
mysql_config = {
    'host' :"db-hqb54.pub-cdb.ntruss.com",
    'user' : "wordwarrior",
    'password' : "wordwarrior9876!",
    'database' : "kordata",
}

"""# json데이터 출력 코드"""

# 최종 클러스터링 결과 json 출력 코드 _ DB에서 불러와서

cluster_mode = 'summary'
# 불용어 로드
with open('../Data/stopwords.txt', 'r') as f:
    stopwords = f.readlines()
    stopwords = [word.strip('\n') for word in stopwords]

# 문자열 연결 함수
def concatenate_strings(strings_list):
    return ''.join(strings_list)

# 파라미터 고정
min_cluster_size = 5
min_samples = 3
n_components = 10

# 클러스터링 대상 일자
dates = ['2023-08-24']


for date in dates:

    connection = pymysql.connect(**mysql_config)

    try:
        # 데이터를 받아올 SQL 쿼리 작성
        query = "SELECT nid, pid, title, datetime, summary FROM news WHERE DATE(datetime) = %s;"
        # cursor.execute(query, date)  # 적절한 값으로 대체
        with connection.cursor() as cursor:
            cursor.execute(query, (date,))
            result = cursor.fetchall()

            # Pandas DataFrame 생성
            df = pd.DataFrame(result, columns=['nid', 'pid', 'title', 'datetime', 'summary'])

    except mysql.connector.Error as err:
        print(f"Error: {err}")

    finally:
        # 연결 종료
        connection.close()

    corpus = df[cluster_mode].values.tolist()
    corpus_embeddings = model.encode(corpus, show_progress_bar=True)

    compare_output = pd.DataFrame()

    tot_df = pd.DataFrame()

    docs_df, docs_per_topic, cluster = hdbscan_process(corpus, corpus_embeddings,
                                            umap=True, n_components=n_components,  # True when umap is used to reduce computation
                                            method='leaf',
                                            min_cluster_size=min_cluster_size,
                                            min_samples=min_samples,
                                            )
    cnt = len(docs_df)

    rslt = [docs_df]
    topics = [docs_per_topic]
    df['cluster'] = docs_df['Topic'].values.tolist()
    tot_df = pd.concat([tot_df, df])

    # 클러스터 결과 업데이트
    df['cluster'] = tot_df['cluster'].astype(str)

    # tf_idf, count = c_tf_idf(topics[0].Doc.values, m=len(corpus))

    # top_n_words = extract_top_n_words_per_topic(tf_idf, count, topics[0], n=30)
    topic_sizes = extract_topic_sizes(rslt[0])


    filtered_df = topic_sizes[topic_sizes['Topic'] != -1]

    # 소속 기사 수 기준 상위 5개 클러스터
    first_largest_index = filtered_df.index[0]
    first_largest_topic = filtered_df.loc[first_largest_index, 'Topic']
    second_largest_index = filtered_df.index[1]
    second_largest_topic = filtered_df.loc[second_largest_index, 'Topic']
    third_largest_index = filtered_df.index[2]
    third_largest_topic = filtered_df.loc[third_largest_index, 'Topic']
    fourth_largest_index = filtered_df.index[3]
    fourth_largest_topic = filtered_df.loc[fourth_largest_index, 'Topic']
    fifth_largest_index = filtered_df.index[4]
    fifth_largest_topic = filtered_df.loc[fifth_largest_index, 'Topic']

    clusters = [first_largest_topic,second_largest_topic,third_largest_topic,fourth_largest_topic,fifth_largest_topic]
    number = 1
    rank_cluster_list = []
    cluster_DB_list = []

    # 상위 5개 클러스터에 대해 출력 데이터 생성
    for i, cluster in enumerate(clusters):

        rank_cluster_num = round(cluster)
        rank_cluster = str(round(cluster))
        rank_cluster_list.append(rank_cluster)

        condition = (df.cluster == rank_cluster)
        rank_cluster_news_titles = df[condition].title.values.tolist()

        # 클러스터의 소속 기사 리스트
        nid_number_output = df.copy()
        nid_number_output.rename(columns={'cluster': 'number'}, inplace=True)
        nid_number_output.drop(['title', 'datetime','summary'], axis=1, inplace=True)
        nid_number_output_rank = nid_number_output[nid_number_output['number'] == rank_cluster]
        nid_DB_list = nid_number_output_rank['nid'].to_list()

        # 클러스터 키워드 추출
        kws_by_cluster = extract_kws(nid_DB_list)

        nid_DB_list = [str(item) for item in nid_DB_list]

        # 대표기사 제목 선정
        rank_best_title = best_title(rank_cluster_news_titles, kws_by_cluster)

        rank_cluster_keyword_join = ','.join(kws_by_cluster)

        # 출력 데이터
        sub_DB_dict = {}

        sub_DB_dict = {'number': f"{number}", 'datetime': date, 'keyword': rank_cluster_keyword_join, 'best_title': rank_best_title, 'nid': nid_DB_list}
        cluster_DB_list.append(sub_DB_dict)

        number += 1

    # JSON 형식으로 출력
    filepath = f'/cluster_{date}.json'
    with open(filepath, "w", encoding="utf-8") as json_file:
        json.dump(cluster_DB_list, json_file, ensure_ascii=False)

    print(date)

